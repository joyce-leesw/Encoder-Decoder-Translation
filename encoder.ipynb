{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder decoder translation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from lib import clean_sentence, tokenise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"./data/fra.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "translation_file = open(path_to_data, \"r\", encoding='utf-8')\n",
    "raw = translation_file.read()\n",
    "translation_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = raw.split('\\n')\n",
    "pairs = [sentence.split('\\t') for sentence in raw[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "english_sen = [clean_sentence(pair[0]) for pair in pairs]\n",
    "french_sen = [clean_sentence(pair[1]) for pair in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise the sentence\n",
    "fre_txt_tokenised, fre_txt_tokeniser = tokenise(french_sen)\n",
    "eng_txt_tokenised, eng_txt_tokeniser = tokenise(english_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of French sentence: 55\n",
      "Max length of English sentence: 47\n"
     ]
    }
   ],
   "source": [
    "print(f'Max length of French sentence: {len(max(fre_txt_tokenised, key=len))}')\n",
    "print(f'Max length of English sentence: {len(max(eng_txt_tokenised, key=len))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French vocab is of 37638\n",
      "English vocab is of 16447\n"
     ]
    }
   ],
   "source": [
    "fre_vocab = len(fre_txt_tokeniser.word_index) + 1\n",
    "eng_vocab = len(eng_txt_tokeniser.word_index) + 1\n",
    "print(f\"French vocab is of {fre_vocab}\")\n",
    "print(f\"English vocab is of {eng_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find max length of all the sentences in each language\n",
    "max_fre_len = int(len(max(fre_txt_tokenised, key=len)))\n",
    "max_eng_len = int(len(max(eng_txt_tokenised, key=len)))\n",
    "\n",
    "# add padding at the end of the sequences so that sentence of the same language all have the same length\n",
    "fre_pad_sen = pad_sequences(fre_txt_tokenised, max_fre_len, padding = \"post\")\n",
    "eng_pad_sen = pad_sequences(eng_txt_tokenised, max_eng_len, padding = \"post\")\n",
    "\n",
    "# reshape data, add a dimension\n",
    "fre_pad_sen = fre_pad_sen.reshape(fre_pad_sen.shape[0], fre_pad_sen.shape[1], 1)\n",
    "eng_pad_sen = eng_pad_sen.reshape(eng_pad_sen.shape[0], eng_pad_sen.shape[1], 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input embedding\n",
    "input_sequence = Input(shape=(max_fre_len, ))\n",
    "embedding = Embedding(input_dim=fre_vocab, output_dim=128, )(input_sequence)\n",
    "# LSTM layer of encoder\n",
    "encoder = LSTM(64, return_sequences=False)(embedding)\n",
    "# hidden layer\n",
    "r_vec = RepeatVector(max_eng_len)(encoder)\n",
    "# LSTM layer of decoder, return sequence is True bc we want the output vector at each timestep\n",
    "decoder = LSTM(64, return_sequences = True, dropout = 0.2)(r_vec)\n",
    "# output layer\n",
    "logits = TimeDistributed(Dense(eng_vocab))(decoder)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input layer is a sequence of int representing indices of input words in french"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output dim of the embedding vector converts any French word into a vector of shape of the specified output dim where each dim represents a characteristic defining the word. The higher dim, the more semantic meaning and the more calculations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each hidden state represents a summary of the input sequence up to the corresponding time step. We only want the final hidden state which is the last time step of the input sequence due to reduced computational complexity, variable length inputs and better performance. Hidden vector is repeated n times using RepeatVector so LSTM in the decoder layer receives the same vector. N is defined here as the number of time steps in the decoder, which is the max English sentence length."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense layer is used to predict translated word. The shape of output vector is the length of English vocabulary because it represents the probability distribution over every possible English word, resulting in all values becoming close to 0 except 1. The index of unit that outputs a 1 will then be mapped to a dictionary for a word. It predicts one word at a time using TimeDistributed and it applies same Dense layer to every time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 55)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 55, 128)           4817664   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                49408     \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 47, 64)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 47, 64)            33024     \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 47, 16447)        1069055   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 47, 16447)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,969,151\n",
      "Trainable params: 5,969,151\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# stack layers to create a model\n",
    "enc_dec_model = Model(input_sequence, Activation('softmax')(logits))\n",
    "enc_dec_model.compile(loss=sparse_categorical_crossentropy,\n",
    "                      optimizer = Adam(1e-3),\n",
    "                      metrics=['accuracy'])\n",
    "enc_dec_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "6529/6529 [==============================] - 3481s 532ms/step - loss: 0.9431 - accuracy: 0.8757\n",
      "Epoch 2/30\n",
      "6529/6529 [==============================] - 3433s 526ms/step - loss: 0.8663 - accuracy: 0.8765\n",
      "Epoch 3/30\n",
      "6529/6529 [==============================] - 3436s 526ms/step - loss: 0.8661 - accuracy: 0.8764\n",
      "Epoch 4/30\n",
      "6529/6529 [==============================] - 3418s 524ms/step - loss: 0.8340 - accuracy: 0.8777\n",
      "Epoch 5/30\n",
      "6529/6529 [==============================] - 3450s 528ms/step - loss: 0.7477 - accuracy: 0.8838\n",
      "Epoch 6/30\n",
      "6529/6529 [==============================] - 3412s 523ms/step - loss: 0.6561 - accuracy: 0.8948\n",
      "Epoch 7/30\n",
      "6529/6529 [==============================] - 3408s 522ms/step - loss: 0.5898 - accuracy: 0.9017\n",
      "Epoch 8/30\n",
      "6529/6529 [==============================] - 3447s 528ms/step - loss: 0.5413 - accuracy: 0.9060\n",
      "Epoch 9/30\n",
      "6529/6529 [==============================] - 3556s 545ms/step - loss: 0.5051 - accuracy: 0.9090\n",
      "Epoch 10/30\n",
      "6529/6529 [==============================] - 3556s 545ms/step - loss: 0.4759 - accuracy: 0.9115\n",
      "Epoch 11/30\n",
      "6529/6529 [==============================] - 3551s 544ms/step - loss: 0.4520 - accuracy: 0.9137\n",
      "Epoch 12/30\n",
      "6529/6529 [==============================] - 3538s 542ms/step - loss: 0.4324 - accuracy: 0.9156\n",
      "Epoch 13/30\n",
      "6529/6529 [==============================] - 3543s 543ms/step - loss: 0.4159 - accuracy: 0.9173\n",
      "Epoch 14/30\n",
      "6529/6529 [==============================] - 3541s 542ms/step - loss: 0.4019 - accuracy: 0.9188\n",
      "Epoch 15/30\n",
      "6529/6529 [==============================] - 3531s 541ms/step - loss: 0.3901 - accuracy: 0.9202\n",
      "Epoch 16/30\n",
      "6529/6529 [==============================] - 3530s 541ms/step - loss: 0.3796 - accuracy: 0.9214\n",
      "Epoch 17/30\n",
      "6529/6529 [==============================] - 3549s 544ms/step - loss: 0.3708 - accuracy: 0.9224\n",
      "Epoch 18/30\n",
      "6529/6529 [==============================] - 3514s 538ms/step - loss: 0.3629 - accuracy: 0.9234\n",
      "Epoch 19/30\n",
      "6529/6529 [==============================] - 27955s 4s/step - loss: 0.3557 - accuracy: 0.9244\n",
      "Epoch 20/30\n",
      "6529/6529 [==============================] - 3436s 526ms/step - loss: 0.3494 - accuracy: 0.9252\n",
      "Epoch 21/30\n",
      "6529/6529 [==============================] - 3424s 524ms/step - loss: 0.3435 - accuracy: 0.9259\n",
      "Epoch 22/30\n",
      "6529/6529 [==============================] - 3418s 523ms/step - loss: 0.3383 - accuracy: 0.9267\n",
      "Epoch 23/30\n",
      "6529/6529 [==============================] - 3430s 525ms/step - loss: 0.3335 - accuracy: 0.9273\n",
      "Epoch 24/30\n",
      "6529/6529 [==============================] - 3399s 521ms/step - loss: 0.3287 - accuracy: 0.9280\n",
      "Epoch 25/30\n",
      "6529/6529 [==============================] - 3399s 521ms/step - loss: 0.3246 - accuracy: 0.9286\n",
      "Epoch 26/30\n",
      "6529/6529 [==============================] - 3472s 532ms/step - loss: 0.3211 - accuracy: 0.9291\n",
      "Epoch 27/30\n",
      "6529/6529 [==============================] - 3502s 536ms/step - loss: 0.3174 - accuracy: 0.9297\n",
      "Epoch 28/30\n",
      "6529/6529 [==============================] - 3528s 540ms/step - loss: 0.3141 - accuracy: 0.9301\n",
      "Epoch 29/30\n",
      "6529/6529 [==============================] - 3480s 533ms/step - loss: 0.3110 - accuracy: 0.9306\n",
      "Epoch 30/30\n",
      "6529/6529 [==============================] - 3479s 533ms/step - loss: 0.3083 - accuracy: 0.9310\n"
     ]
    }
   ],
   "source": [
    "model_output = enc_dec_model.fit(fre_pad_sen, eng_pad_sen, batch_size = 32, epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_dec_model.save(\"enc_dec-LSTM.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The English sentence is hi\n",
      "The French sentence is: salut\n"
     ]
    }
   ],
   "source": [
    "def logits_to_sentence(logits, tokeniser):\n",
    "    index_to_words = {idx: word for word, idx in tokeniser.word_index.items()}\n",
    "    index_to_words[0] = '<empty>'\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "index = 5\n",
    "print(f\"The English sentence is {english_sen[index]}\")\n",
    "print(f\"The French sentence is: {french_sen[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "hello <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty> <empty>\n"
     ]
    }
   ],
   "source": [
    "print(logits_to_sentence(enc_dec_model.predict(fre_pad_sen[index:index+1])[0], eng_txt_tokeniser))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fre_word = \"Je suis un chat\"\n",
    "\n",
    "fre_seq = fre_txt_tokeniser.texts_to_sequences([fre_word])\n",
    "fre_pad_seq = pad_sequences(fre_seq, maxlen=max_fre_len, padding='post')\n",
    "\n",
    "# Generate the English translation\n",
    "eng_logits = enc_dec_model.predict(fre_pad_seq)\n",
    "eng_sentence = logits_to_sentence(eng_logits[0], eng_txt_tokeniser)\n",
    "\n",
    "# Print the translation\n",
    "print(\"French word:\", fre_word)\n",
    "print(\"English translation:\", eng_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
